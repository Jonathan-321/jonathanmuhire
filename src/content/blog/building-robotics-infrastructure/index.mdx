---
title: 'Building Scalable Infrastructure for Robotics Data'
description: 'Lessons learned from architecting data infrastructure for embodied AI and robotics applications'
date: 2024-11-10
tags: ['robotics', 'infrastructure', 'data-engineering']
authors: ['jonathan-muhire']
draft: false
---

import Callout from '@/components/Callout.astro'

One of the biggest challenges in robotics today isn't just making robots work—it's managing the massive amounts of data they generate. At Neotix, we learned this the hard way as we scaled from collecting gigabytes to terabytes of sensor data daily.

## The Data Challenge in Robotics

Modern robots are essentially mobile data collection platforms. A single teleoperation session can generate:

- High-resolution camera feeds (often multiple cameras)
- Joint position and velocity data
- Force/torque sensor readings
- Point cloud data from depth cameras
- Proprioceptive sensor data

This quickly adds up to hundreds of gigabytes per day, per robot.

## Key Infrastructure Components

### 1. Object Storage with MinIO

We chose MinIO as our S3-compatible object storage solution for several reasons:

```python
# Example: Uploading robot data to MinIO
import boto3
from datetime import datetime

s3_client = boto3.client('s3',
    endpoint_url='http://minio:9000',
    aws_access_key_id='your-access-key',
    aws_secret_access_key='your-secret-key'
)

def upload_robot_session(session_data, robot_id):
    timestamp = datetime.now().isoformat()
    key = f"sessions/{robot_id}/{timestamp}/data.zarr"
    
    s3_client.upload_file(
        session_data,
        'robot-data',
        key
    )
```

### 2. Version Control with LakeFS

LakeFS gave us Git-like semantics for our data:

- Branch different experiments
- Rollback corrupted datasets
- Track data lineage
- Collaborate without conflicts

<Callout type="info">
LakeFS branches are zero-copy, meaning you can create thousands of experimental branches without duplicating data.
</Callout>

### 3. Data Pipeline Architecture

Our pipeline follows these principles:

1. **Immediate upload**: Data is uploaded to object storage as soon as collection completes
2. **Metadata first**: We track metadata separately for fast querying
3. **Lazy processing**: Heavy processing happens on-demand, not during collection
4. **Format standardization**: Everything is stored in Zarr format for efficient access

## Lessons Learned

### Start with Standards
Choose standard formats early. We standardized on:
- Zarr for array data
- Parquet for tabular metadata
- MP4 for video (with separate pose data)

### Plan for Scale
What works for 10GB won't work for 10TB. Design with these assumptions:
- Network will be your bottleneck
- You'll need parallel processing
- Metadata queries must be fast
- Storage costs will dominate

### Version Everything
Not just code—version your:
- Data schemas
- Processing pipelines
- Model checkpoints
- Configuration files

## Looking Forward

The future of robotics infrastructure lies in:

1. **Multimodal indexing**: Searching across video, sensor data, and language
2. **Active learning pipelines**: Automatically identifying valuable data
3. **Sim-to-real bridges**: Seamless integration of simulated and real data
4. **Edge computing**: Processing data closer to the robot

Building robust data infrastructure isn't glamorous, but it's the foundation that enables everything else in robotics. Without it, even the best algorithms will fail to scale.

*What infrastructure challenges have you faced in robotics? I'd love to hear your experiences—reach out at muhirejonathan123@gmail.com*